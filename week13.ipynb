{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week13.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hg9E5vhwejWv",
        "outputId": "519c0df4-f397-46e6-bbae-3aafc0234f30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "from numpy import newaxis\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers.recurrent import LSTM, GRU\n",
        "from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "print ('import completed')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GzwDN3Qemkr",
        "outputId": "af9911a3-3c87-4ca2-af18-22860f63117f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter in how much steps we will enroll the network.\n",
        "# RNN/LSTM/GRU can be taught patterns over times series as big as the number of times you enrol them, and no bigger (fundamental limitation). \n",
        "# So by design these networks are deep/long to catch recurrent patterns.\n",
        "Enrol_window = 100\n",
        "\n",
        "print ('enrol window set to',Enrol_window )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUDlA7Fmen7z",
        "outputId": "cc99e767-801a-4109-8277-c93c66e69c58"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enrol window set to 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Support functions\n",
        "sc = MinMaxScaler(feature_range=(0,1))\n",
        "def load_data(datasetname, column, seq_len, normalise_window):\n",
        "    # A support function to help prepare datasets for an RNN/LSTM/GRU\n",
        "    data = datasetname.loc[:,column]\n",
        "\n",
        "    sequence_length = seq_len + 1\n",
        "    result = []\n",
        "    for index in range(len(data) - sequence_length):\n",
        "        result.append(data[index: index + sequence_length])\n",
        "    \n",
        "    if normalise_window:\n",
        "        #result = sc.fit_transform(result)\n",
        "        result = normalise_windows(result)\n",
        "\n",
        "    result = np.array(result)\n",
        "\n",
        "    #Last 10% is used for validation test, first 90% for training\n",
        "    row = round(0.9 * result.shape[0])\n",
        "    train = result[:int(row), :]\n",
        "    np.random.shuffle(train)\n",
        "    x_train = train[:, :-1]\n",
        "    y_train = train[:, -1]\n",
        "    x_test = result[int(row):, :-1]\n",
        "    y_test = result[int(row):, -1]\n",
        "\n",
        "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
        "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))  \n",
        "\n",
        "    return [x_train, y_train, x_test, y_test]\n",
        "\n",
        "def normalise_windows(window_data):\n",
        "    # A support function to normalize a dataset\n",
        "    normalised_data = []\n",
        "    for window in window_data:\n",
        "        normalised_window = [((float(p) / float(window[0])) - 1) for p in window]\n",
        "        normalised_data.append(normalised_window)\n",
        "    return normalised_data\n",
        "\n",
        "def predict_sequence_full(model, data, window_size):\n",
        "    #Shift the window by 1 new prediction each time, re-run predictions on new window\n",
        "    curr_frame = data[0]\n",
        "    predicted = []\n",
        "    for i in range(len(data)):\n",
        "        predicted.append(model.predict(curr_frame[newaxis,:,:])[0,0])\n",
        "        curr_frame = curr_frame[1:]\n",
        "        curr_frame = np.insert(curr_frame, [window_size-1], predicted[-1], axis=0)\n",
        "    return predicted\n",
        "\n",
        "def predict_sequences_multiple(model, data, window_size, prediction_len):\n",
        "    #Predict sequence of <prediction_len> steps before shifting prediction run forward by <prediction_len> steps\n",
        "    prediction_seqs = []\n",
        "    for i in range(int(len(data)/prediction_len)):\n",
        "        curr_frame = data[i*prediction_len]\n",
        "        predicted = []\n",
        "        for j in range(prediction_len):\n",
        "            predicted.append(model.predict(curr_frame[newaxis,:,:])[0,0])\n",
        "            curr_frame = curr_frame[1:]\n",
        "            curr_frame = np.insert(curr_frame, [window_size-1], predicted[-1], axis=0)\n",
        "        prediction_seqs.append(predicted)\n",
        "    return prediction_seqs\n",
        "\n",
        "def plot_results(predicted_data, true_data): \n",
        "    fig = plt.figure(facecolor='white') \n",
        "    ax = fig.add_subplot(111) \n",
        "    ax.plot(true_data, label='True Data') \n",
        "    plt.plot(predicted_data, label='Prediction') \n",
        "    plt.legend() \n",
        "    plt.show() \n",
        "    \n",
        "def plot_results_multiple(predicted_data, true_data, prediction_len):\n",
        "    fig = plt.figure(facecolor='white')\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.plot(true_data, label='True Data')\n",
        "    #Pad the list of predictions to shift it in the graph to it's correct start\n",
        "    for i, data in enumerate(predicted_data):\n",
        "        padding = [None for p in range(i * prediction_len)]\n",
        "        plt.plot(padding + data, label='Prediction')\n",
        "        plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "print ('Support functions defined')# Support functions\n",
        "sc = MinMaxScaler(feature_range=(0,1))\n",
        "def load_data(datasetname, column, seq_len, normalise_window):\n",
        "    # A support function to help prepare datasets for an RNN/LSTM/GRU\n",
        "    data = datasetname.loc[:,column]\n",
        "\n",
        "    sequence_length = seq_len + 1\n",
        "    result = []\n",
        "    for index in range(len(data) - sequence_length):\n",
        "        result.append(data[index: index + sequence_length])\n",
        "    \n",
        "    if normalise_window:\n",
        "        #result = sc.fit_transform(result)\n",
        "        result = normalise_windows(result)\n",
        "\n",
        "    result = np.array(result)\n",
        "\n",
        "    #Last 10% is used for validation test, first 90% for training\n",
        "    row = round(0.9 * result.shape[0])\n",
        "    train = result[:int(row), :]\n",
        "    np.random.shuffle(train)\n",
        "    x_train = train[:, :-1]\n",
        "    y_train = train[:, -1]\n",
        "    x_test = result[int(row):, :-1]\n",
        "    y_test = result[int(row):, -1]\n",
        "\n",
        "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
        "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))  \n",
        "\n",
        "    return [x_train, y_train, x_test, y_test]\n",
        "\n",
        "def normalise_windows(window_data):\n",
        "    # A support function to normalize a dataset\n",
        "    normalised_data = []\n",
        "    for window in window_data:\n",
        "        normalised_window = [((float(p) / float(window[0])) - 1) for p in window]\n",
        "        normalised_data.append(normalised_window)\n",
        "    return normalised_data\n",
        "\n",
        "def predict_sequence_full(model, data, window_size):\n",
        "    #Shift the window by 1 new prediction each time, re-run predictions on new window\n",
        "    curr_frame = data[0]\n",
        "    predicted = []\n",
        "    for i in range(len(data)):\n",
        "        predicted.append(model.predict(curr_frame[newaxis,:,:])[0,0])\n",
        "        curr_frame = curr_frame[1:]\n",
        "        curr_frame = np.insert(curr_frame, [window_size-1], predicted[-1], axis=0)\n",
        "    return predicted\n",
        "\n",
        "def predict_sequences_multiple(model, data, window_size, prediction_len):\n",
        "    #Predict sequence of <prediction_len> steps before shifting prediction run forward by <prediction_len> steps\n",
        "    prediction_seqs = []\n",
        "    for i in range(int(len(data)/prediction_len)):\n",
        "        curr_frame = data[i*prediction_len]\n",
        "        predicted = []\n",
        "        for j in range(prediction_len):\n",
        "            predicted.append(model.predict(curr_frame[newaxis,:,:])[0,0])\n",
        "            curr_frame = curr_frame[1:]\n",
        "            curr_frame = np.insert(curr_frame, [window_size-1], predicted[-1], axis=0)\n",
        "        prediction_seqs.append(predicted)\n",
        "    return prediction_seqs\n",
        "\n",
        "def plot_results(predicted_data, true_data): \n",
        "    fig = plt.figure(facecolor='white') \n",
        "    ax = fig.add_subplot(111) \n",
        "    ax.plot(true_data, label='True Data') \n",
        "    plt.plot(predicted_data, label='Prediction') \n",
        "    plt.legend() \n",
        "    plt.show() \n",
        "    \n",
        "def plot_results_multiple(predicted_data, true_data, prediction_len):\n",
        "    fig = plt.figure(facecolor='white')\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.plot(true_data, label='True Data')\n",
        "    #Pad the list of predictions to shift it in the graph to it's correct start\n",
        "    for i, data in enumerate(predicted_data):\n",
        "        padding = [None for p in range(i * prediction_len)]\n",
        "        plt.plot(padding + data, label='Prediction')\n",
        "        plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "print ('Support functions defined')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldL9xL1Ceplr",
        "outputId": "a2712ce7-d1ef-48f8-9558-8f3d82c39144"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Support functions defined\n",
            "Support functions defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "dataset = pd.read_csv('drive/MyDrive/dataset/Sin Wave Data Generator.csv')\n",
        "dataset[\"Wave\"][:].plot(figsize=(16,4),legend=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "SgaN3MWMesMJ",
        "outputId": "5e230592-0496-4540-975b-f7f35ba4f243"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-a9d97bb8bd88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drive/MyDrive/dataset/Sin Wave Data Generator.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Wave\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/MyDrive/dataset/Sin Wave Data Generator.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the dataset, note that all data foer the sinus wave is already normalized between 0 and 1\n",
        "# A label is the thing we're predicting\n",
        "# A feature is an input variable, in this case a stock price\n",
        "\n",
        "feature_train, label_train, feature_test, label_test = load_data(dataset, 'Wave', Enrol_window, False)\n",
        "\n",
        "print ('Datasets generated')"
      ],
      "metadata": {
        "id": "-dzELdHfetZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The LSTM model I would like to test\n",
        "# Note: replace LSTM with GRU or RNN if you want to try those\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(feature_train.shape[1],1)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100, return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation = \"linear\"))\n",
        "\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "print ('model compiled')\n",
        "\n",
        "print (model.summary())"
      ],
      "metadata": {
        "id": "pLEGtbLsevR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "model.fit(feature_train, label_train, batch_size=512, epochs=10, validation_data = (feature_test, label_test))"
      ],
      "metadata": {
        "id": "ChQlX96XewmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's use the model and predict the wave\n",
        "predictions= predict_sequence_full(model, feature_test, Enrol_window)\n",
        "plot_results(predictions,label_test)"
      ],
      "metadata": {
        "id": "-aR7rKdCeyRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get the stock data\n",
        "dataset = pd.read_csv('drive/MyDrive/dataset/IBM_2006-01-01_to_2018-01-01.csv', index_col='Date', parse_dates=['Date'])\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "81zjoTqvezlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the dataset, note that the stock price data will be normalized between 0 and 1\n",
        "# A label is the thing we're predicting\n",
        "# A feature is an input variable, in this case a stock price\n",
        "# Selected 'Close' (stock pric at closing) attribute for prices. Let's see what it looks like\n",
        "\n",
        "feature_train, label_train, feature_test, label_test = load_data(dataset, 'Close', Enrol_window, True)\n",
        "\n",
        "dataset[\"Close\"][:'2016'].plot(figsize=(16,4),legend=True)\n",
        "dataset[\"Close\"]['2017':].plot(figsize=(16,4),legend=True) # 10% is used for thraining data which is approx 2017 data\n",
        "plt.legend(['Training set (First 90%, approx before 2017)','Test set (Last 10%, approax 2017 and beyond)'])\n",
        "plt.title('IBM stock price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sBymuZske09X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The same LSTM model I would like to test, lets see if the sinus prediction results can be matched\n",
        "# Note: replace LSTM with GRU or RNN if you want to try those\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(feature_train.shape[1],1)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100, return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation = \"linear\"))\n",
        "\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "print ('model compiled')\n",
        "\n",
        "print (model.summary())"
      ],
      "metadata": {
        "id": "8RjwEDQbe2UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "model.fit(feature_train, label_train, batch_size=512, epochs=5, validation_data = (feature_test, label_test))"
      ],
      "metadata": {
        "id": "fgRRUCm9e3vM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's use the model and predict the stock\n",
        "predicted_stock_price = model.predict(feature_test)\n",
        "plot_results(predicted_stock_price,label_test)"
      ],
      "metadata": {
        "id": "Nw802JCee47-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = predict_sequences_multiple(model, feature_test, Enrol_window, 50)\n",
        "plot_results_multiple(predictions, label_test, 50)"
      ],
      "metadata": {
        "id": "miql1EGre575"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}